{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5518f5",
   "metadata": {},
   "source": [
    "# Notebook Walkthrough of Database Creation + Processing\n",
    "\n",
    "As seen in the image below, this notebook just walks through the assumptions, filters and processing steps taken to create the SQL database used in the main analysis notebook.\n",
    "\n",
    "In specific, this notebook walks us through the following steps:\n",
    "1. Data Sources\n",
    "2. Website APIs\n",
    "3. Key Matching\n",
    "4. Data Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972f3fb",
   "metadata": {},
   "source": [
    "<p align=\"center\" width=\"10%\">\n",
    "    <img width=\"50%\" src=\"images/tech_stack.png\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac9150",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-size: 16px; color: #224CA0 ;\">\n",
    "    <strong> Code Architecture Note: </strong>\n",
    "</div>\n",
    "\n",
    "The following cells employ 1 class for API interfacing, requests and SQL table curation. Huge thank you to vishaalagartha and swar for their github repos that already provided the webscraping framework needed to easily extract all the needed data.\n",
    "\n",
    "To see the API commands used, please see the 'Database_Operations.py' file.\n",
    "\n",
    "With that being said, the first step before is of course importing the helper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Helper functions for databases extraction (steps )\n",
    "import sqlite3\n",
    "import Database_Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6163f",
   "metadata": {},
   "source": [
    "# Step 1 - Data Sources\n",
    "The next step is specify the desired seasons of interest (in this study, that is the past 10 years going back to the 2011-2012 season). We then initialize the API_Handlers class to perform the web scraping and queries needed to agglomerate the data.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: left; font-size: 22px; color: #7a1019 ;\">\n",
    "    <strong> Side Note: </strong>\n",
    "</div>\n",
    "\n",
    "I'm not sure if I am just missing something but why does nba.com not provide player salaries!? The whole reason SQL db key matching was needed is because I had to pool multiple sources to get player salaries and stats. The worst part is the naming of players was inconsistent even on a given website. To give an idea of the mismatches, here are a few examples that I was able to systematically correct with string matching techniques:\n",
    "\n",
    "| Mismatch | BBall Ref | NBA.com |\n",
    "|-----------------|-----------------|-----------------|\n",
    "| Special Characters | JokiÄ‡ | Jokic |\n",
    "| Name Suffix | Jaren Jackson Jr. | Jaren Jackson |\n",
    "| Suffix Annotation | Gary Trent Jr. | Gary Trent Jr |\n",
    "| Abbreviations | JJ Barea | Jose Juan Barea Mora |\n",
    "\n",
    "Yes regular expression could probably solve a lot of the problems but using a much faster/easier method, I was able to get salary/player matching error down to <strong> 4.5% </strong> which is pretty good in my opinion. I analyze this error metric a little more in the main notebook by the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5787a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define years of interest\n",
    "valid_years = [str(1999 + i) for i in range(25)]\n",
    "\n",
    "#Initialize API_Handler\n",
    "API_Handler = Database_functions.API_Handlers(valid_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c270dac",
   "metadata": {},
   "source": [
    "# Step 2 - String Matching to Link Multiple Data Sources (Website APIs)\n",
    "The next step is string match player names. I created an automated function that lets you say y/n to a player match but a lot of it was just automated based on if a string match was above or below 80% scoring and the player salary. A lot of the mismatches come from players who basically earned 0$ on non-guaranteed contracts, 10 days, G league call ups, etc. Its pretty easy to just remove these datapoints and replace them with generic placeholder values since the overall influence is negligble on team building from a salary standpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b199e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get matched player salaries from hoopshype\n",
    "ref_players,df_sal = API_Handler.import_hoopshype_data()\n",
    "\n",
    "#Make sure all stats are matched based on a salary cutoff and assumptions mentioned above.\n",
    "player_data = API_Handler.player_matching(ref_players,\n",
    "                                          api_players,\n",
    "                                          df_sal,\n",
    "                                          plot=True,\n",
    "                                          cutoff=2500000,\n",
    "                                          pickler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711124e",
   "metadata": {},
   "source": [
    "# Step 3 - Data Storage \n",
    "The last step before we get into the fun analysis part is creating a sql db and its tables. The key matching structure is visualized below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb527178",
   "metadata": {},
   "source": [
    "<p align=\"center\" width=\"10%\">\n",
    "    <img width=\"70%\" src=\"images/sql_keys.png\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15512e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SQL db \n",
    "API_Handler.generate_sqlite3_db('player_stats','NBA_stats_ghub.db')\n",
    "\n",
    "#create supporting tables\n",
    "API_Handler.generate_player_stats_table('Player_stats', player_data)\n",
    "#Other tables dont need the df since the class handles the explicit generation of the data already\n",
    "API_Handler.generate_salary_table('Player_Salaries')\n",
    "API_Handler.generate_team_table('Team_stats')\n",
    "\n",
    "#create a temporary table to varify real team salaries vs. calculated salaries\n",
    "API_Handler.generate_team_caps_table('team_caps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17335cbc",
   "metadata": {},
   "source": [
    "Now lets move on to the interesting part... the data analysis to see what we can extract from simple data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
